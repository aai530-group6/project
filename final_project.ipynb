{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AAI-530 - Final Team Project - Group 6"
      ],
      "metadata": {
        "id": "XB0TEPry-S_u"
      },
      "id": "XB0TEPry-S_u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "This notebook documents the development of a machine learning IoT application for predicting sleep quality using a Fitbit dataset. The project encompasses data loading, exploratory data analysis, feature engineering, model building (including a deep learning LSTM model and a traditional machine learning classifier), and hyperparameter tuning.\n",
        "\n",
        "## Objectives\n",
        "* To predict the `overall_score` of sleep quality using LSTM, satisfying both the deep learning and time series requirements of the project.\n",
        "* To classify sleep quality features as bad, average, or good for daily notifications on a smartwatch using a traditional ML classifier."
      ],
      "metadata": {
        "id": "9tWXFn4CUZm8"
      },
      "id": "9tWXFn4CUZm8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PROJECT CONSTANTS"
      ],
      "metadata": {
        "id": "NYONxnlwIu3e"
      },
      "id": "NYONxnlwIu3e"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------------- #\n",
        "# GOOGLE DRIVE\n",
        "# ---------------------------------------------------------------------------- #\n",
        "GOOGLE_DRIVE_FOLDER_PATH  = \"530 Final/IoT AAI-530 Final Project\"\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# GITHUB\n",
        "# ---------------------------------------------------------------------------- #\n",
        "REPO_DIR = \"project\"\n",
        "REPO_URL = \"https://github.com/aai530-group6/project.git\"\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# DATASET\n",
        "# ---------------------------------------------------------------------------- #\n",
        "DATASET_FILENAME = \"sleep_score_data_fitbit.csv\"\n",
        "FITBIT_SLEEP_SCORE_DATASET_URL = f\"https://huggingface.co/datasets/aai530-group6/sleep-score-fitbit/resolve/main/{DATASET_FILENAME}?download=true\""
      ],
      "metadata": {
        "id": "W_cGbgAUIWe9"
      },
      "id": "W_cGbgAUIWe9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INSTALLS"
      ],
      "metadata": {
        "id": "kfP-GXtmLLGz"
      },
      "id": "kfP-GXtmLLGz"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get install wkhtmltopdf mono-complete\n",
        "\n",
        "pip install --quiet --progress-bar off \\\n",
        "    black[jupyter] \\\n",
        "    dataprep \\\n",
        "    huggingface-hub \\\n",
        "    isort \\\n",
        "    pdfkit \\\n",
        "    pythonnet \\\n",
        "    wkhtmltopdf"
      ],
      "metadata": {
        "id": "N-9BNrMMLJgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05b0028b-5811-4010-eeda-9585925d3b73"
      },
      "id": "N-9BNrMMLJgE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "mono-complete is already the newest version (6.8.0.105+dfsg-3.2).\n",
            "wkhtmltopdf is already the newest version (0.12.6-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.21.0 requires sqlalchemy<3.0dev,>=1.4, but you have sqlalchemy 1.3.24 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.3.24 which is incompatible.\n",
            "panel 1.3.8 requires bokeh<3.4.0,>=3.2.0, but you have bokeh 2.4.3 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORTS"
      ],
      "metadata": {
        "id": "nz9cgfqjKzLG"
      },
      "id": "nz9cgfqjKzLG"
    },
    {
      "cell_type": "code",
      "source": [
        "import contextlib\n",
        "import hashlib\n",
        "import os\n",
        "import pathlib\n",
        "import zipfile\n",
        "\n",
        "import clr\n",
        "import google.colab\n",
        "import isort\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdfkit\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import System\n",
        "from dataprep.datasets import load_dataset\n",
        "from dataprep.eda import *\n",
        "from dataprep.eda import (create_report, plot, plot_correlation, plot_diff,\n",
        "                          plot_missing)\n",
        "from EvoPdf import HtmlToPdfConverter\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2"
      ],
      "metadata": {
        "id": "lGwe-UxzKwjo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "9b9d7964-d328-4160-8832-e7d6f3e9ce5a"
      },
      "id": "lGwe-UxzKwjo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/clr_loader/mono.py:180: UserWarning: Hosting Mono versions before v6.12 is known to be problematic. If the process crashes shortly after you see this message, try updating Mono to at least v6.12.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'EvoPdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ac006a8106ca>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m from dataprep.eda import (create_report, plot, plot_correlation, plot_diff,\n\u001b[1;32m     20\u001b[0m                           plot_missing)\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mEvoPdf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHtmlToPdfConverter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'EvoPdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT SORTER\n",
        "import_string = \"\"\"\n",
        "import contextlib\n",
        "import hashlib\n",
        "import os\n",
        "import pathlib\n",
        "import zipfile\n",
        "\n",
        "import clr\n",
        "import google.colab\n",
        "import isort\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdfkit\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import System\n",
        "from dataprep.datasets import load_dataset\n",
        "from dataprep.eda import *\n",
        "from dataprep.eda import (create_report, plot, plot_correlation, plot_diff,\n",
        "                          plot_missing)\n",
        "from EvoPdf import HtmlToPdfConverter\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\"\"\"\n",
        "print(isort.code(import_string))"
      ],
      "metadata": {
        "id": "3mWEWrHjL_6H"
      },
      "id": "3mWEWrHjL_6H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HELPER FUNCTIONS"
      ],
      "metadata": {
        "id": "ij93E8xkIzMP"
      },
      "id": "ij93E8xkIzMP"
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip(zip_file_path: str, extraction_directory: str) -> None:\n",
        "    \"\"\"\n",
        "    Unzips a zip file into a specified directory, checking if the zip file\n",
        "    exists before extraction.\n",
        "\n",
        "    :param zip_file_path: Path to the zip file.\n",
        "    :type zip_file_path: str\n",
        "    :param extraction_directory: Target directory for extraction.\n",
        "    :type extraction_directory: str\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    if os.path.exists(zip_file_path):\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extraction_directory)\n",
        "        print(\"Extraction completed successfully.\")\n",
        "    else:\n",
        "        print(\"The zip file does not exist.\")\n",
        "\n",
        "\n",
        "def compute_hash(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Computes the SHA-256 hash for a given file, reading in binary mode and\n",
        "    processing in blocks for efficiency with large files.\n",
        "\n",
        "    :param file_path: Path to the file for hash computation.\n",
        "    :type file_path: str\n",
        "    :return: Hexadecimal string of the file's SHA-256 hash.\n",
        "    :rtype: str\n",
        "    \"\"\"\n",
        "    sha256_hash = hashlib.sha256()\n",
        "    with open(file_path, 'rb') as f:\n",
        "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
        "            sha256_hash.update(byte_block)\n",
        "    return sha256_hash.hexdigest()\n",
        "\n",
        "\n",
        "def should_download(url: str, save_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Determines if a file should be downloaded by checking its existence and\n",
        "    comparing the content hash with the online version.\n",
        "\n",
        "    :param url: URL of the file to potentially download.\n",
        "    :type url: str\n",
        "    :param save_path: Local path where the file would be saved.\n",
        "    :type save_path: str\n",
        "    :return: True if download is needed, False otherwise.\n",
        "    :rtype: bool\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        # DON'T DOWNLOAD IF INACCESSIBLE\n",
        "        if response.status_code != 200:\n",
        "            return False\n",
        "        # DON'T DOWNLOAD IF SAME FILE\n",
        "        downloaded_hash = hashlib.sha256(response.content).hexdigest()\n",
        "        if os.path.exists(save_path):\n",
        "            existing_hash = compute_hash(save_path)\n",
        "            if existing_hash == downloaded_hash:\n",
        "                return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def download(url: str, save_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Downloads a file from a URL to a local path if it's absent or outdated.\n",
        "    Raises an exception for non-200 HTTP status during download.\n",
        "\n",
        "    :param url: URL of the file to download.\n",
        "    :type url: str\n",
        "    :param save_path: Local path to save the downloaded file.\n",
        "    :type save_path: str\n",
        "    :raises Exception: For non-200 HTTP status during download.\n",
        "    :return: None. Directly writes to disk if download occurs.\n",
        "    :rtype: None\n",
        "    \"\"\"\n",
        "    if should_download(url, save_path):\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(f\"DOWNLOAD FAILED. STATUS: {response.status_code}\")\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "    print(f\"DOWNLOADED FILE: {save_path}.\")\n",
        "\n",
        "\n",
        "def create_google_drive_shortcut(target_folder_name: str = \"temp\") -> str:\n",
        "    \"\"\"\n",
        "    Creates a shortcut to a Google Drive folder, ensuring easier access by\n",
        "    mounting Google Drive and creating a symlink in the Colab environment.\n",
        "\n",
        "    :param target_folder_name: Name of the Google Drive folder for the shortcut.\n",
        "    :type target_folder_name: str\n",
        "    :return: Path to the symlink created in the Colab environment.\n",
        "    :rtype: str\n",
        "    \"\"\"\n",
        "    # MOUNT GOOGLE DRIVE\n",
        "    with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
        "        google.colab.drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "    # DEFINE BASE PATHS FOR DRIVE AND SHORTCUTS\n",
        "    base_drive_path = pathlib.Path(\"/content/drive/My Drive\")\n",
        "    project_path = base_drive_path / target_folder_name\n",
        "    shortcut_folder_name = target_folder_name.split('/')[-1]\n",
        "    shortcut_path = pathlib.Path(f\"/content/{shortcut_folder_name}\")\n",
        "\n",
        "    # ENSURE PROJECT FOLDER AND PARENT FOLDERS EXIST IN GOOGLE DRIVE\n",
        "    project_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # CREATE FOLDER SHORTCUT IF NON-EXISTENT\n",
        "    if not shortcut_path.exists() or not shortcut_path.is_symlink():\n",
        "        if shortcut_path.exists():\n",
        "            shortcut_path.unlink()\n",
        "        shortcut_path.symlink_to(project_path, target_is_directory=True)\n",
        "    print(f\"FOLDER SHORTCUT: {shortcut_path} --> {project_path}\")\n",
        "    return str(shortcut_path)\n",
        "\n",
        "\n",
        "def git_clone(base_dir: str, repo_dir: str, url: str, gh_token: str) -> None:\n",
        "    \"\"\"\n",
        "    Clones a Git repository into a specified directory. If the directory does\n",
        "    not exist, it is created. Uses a GitHub read-only token for authentication.\n",
        "    Prints a message indicating the repository's clone status.\n",
        "\n",
        "    :param base_dir: The base directory for cloning the repository.\n",
        "    :type base_dir: str\n",
        "    :param repo_dir: The directory name for the cloned repository.\n",
        "    :type repo_dir: str\n",
        "    :param url: The HTTP URL of the Git repository to clone.\n",
        "    :type url: str\n",
        "    :param gh_token: The GitHub read-only access token for authentication.\n",
        "    :type gh_token: str\n",
        "    :return: None. Indicate clone repository status at the specified directory.\n",
        "    \"\"\"\n",
        "    clone_path = os.path.join(base_dir, repo_dir)\n",
        "    if not os.path.exists(clone_path):\n",
        "        !git clone {auth_repo_url} {clone_path}\n",
        "    print(f\"CLONED: {clone_path}\")\n"
      ],
      "metadata": {
        "id": "FHf1pgiQCehA"
      },
      "id": "FHf1pgiQCehA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SETUP"
      ],
      "metadata": {
        "id": "mcI3xtPHJHqS"
      },
      "id": "mcI3xtPHJHqS"
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE SAMPLE DATA FOLDER\n",
        "!rm -rf /content/sample_data\n",
        "\n",
        "# CREATE GOOGLE DRIVE FOLDER SHORTCUT FOR SPEED AND PERSISTENCE\n",
        "SHORTCUT  = create_google_drive_shortcut(GOOGLE_DRIVE_FOLDER_PATH)\n",
        "\n",
        "# CLONE GITHUB REPOSITORY\n",
        "CLONE_PATH = os.path.join(SHORTCUT, REPO_DIR)\n",
        "if not os.path.exists(CLONE_PATH):\n",
        "    !cd \"{SHORTCUT}\" && git clone \"{REPO_URL}\"\n",
        "\n",
        "# DOWNLOAD DATASET TO GOOGLE DRIVE\n",
        "DATASET_FILEPATH = os.path.join(SHORTCUT, DATASET_FILENAME)\n",
        "download(FITBIT_SLEEP_SCORE_DATASET_URL, DATASET_FILEPATH)\n",
        "\n",
        "# UNZIP EVO\n",
        "unzip(\"/content/IoT AAI-530 Final Project/project/files/EvoHtmlToPdf-v10.0.zip\", \"/content/EvoHtmlToPdf\")\n",
        "\n",
        "!cp \"/content/EvoHtmlToPdf/evointernal.dat\" \"\""
      ],
      "metadata": {
        "id": "Sw7jBAuKI-XR"
      },
      "id": "Sw7jBAuKI-XR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOAD"
      ],
      "metadata": {
        "id": "e-i2X2MAZsnZ"
      },
      "id": "e-i2X2MAZsnZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from a CSV file into a pandas DataFrame\n",
        "dataset_df = pd.read_csv(DATASET_FILEPATH)\n",
        "\n",
        "# Initialize an empty dictionary to store different versions of DataFrames\n",
        "dfs = {}\n",
        "\n",
        "# Copy the original DataFrame and store in the dictionary with key 'sleep_score'\n",
        "# This keeps the original dataset unchanged while working on its copy\n",
        "dfs['sleep_score'] = dataset_df.copy()\n",
        "\n",
        "# Retrieve and assign the copied DataFrame from the dictionary to 'df'\n",
        "# This allows for easier manipulation of this specific dataset version\n",
        "df = dfs['sleep_score']"
      ],
      "metadata": {
        "id": "m73S8ZNQA9Nk"
      },
      "id": "m73S8ZNQA9Nk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "2xDIH6xPDT5h"
      },
      "id": "2xDIH6xPDT5h"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_missing(df)"
      ],
      "metadata": {
        "id": "Hb_YZLIfZ0Dz"
      },
      "id": "Hb_YZLIfZ0Dz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_correlation(df, \"overall_score\")"
      ],
      "metadata": {
        "id": "ke735m1NZ9HQ"
      },
      "id": "ke735m1NZ9HQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EDA_REPORT_FILENAME = \"exploratory_data_analysis\"\n",
        "report = create_report(df, title=\"Exploratory Data Analysis\")"
      ],
      "metadata": {
        "id": "fy_OGMoKNbLb"
      },
      "id": "fy_OGMoKNbLb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# report.save(f\"{SHORTCUT}/{EDA_REPORT_FILENAME}\")\n",
        "# html =f\"/{SHORTCUT}/{EDA_REPORT_FILENAME}.html\"\n",
        "\n",
        "# import clr\n",
        "# import System\n",
        "# # ADD REFERENCE TO THE EVOPDF DLL\n",
        "# clr.AddReference(\"/content/EvoHtmlToPdf/evohtmltopdf.dll\")\n",
        "# from EvoPdf import HtmlToPdfConverter\n",
        "\n",
        "# # DEFINE THE PATH TO YOUR HTML FILE\n",
        "# html_file_path = f\"/{SHORTCUT}/{EDA_REPORT_FILENAME}.html\"\n",
        "\n",
        "# # READ THE HTML CONTENT FROM THE FILE\n",
        "# with open(html_file_path, 'r', encoding='utf-8') as file:\n",
        "#     html_content = file.read()\n",
        "\n",
        "# # INITIALIZE THE HTML TO PDF CONVERTER\n",
        "# converter = HtmlToPdfConverter()\n",
        "# converter.PdfToolFullPath = \"/content/EvoHtmlToPdf\"\n",
        "# converter.ConversionDelay = 0\n",
        "\n",
        "# pdf_bytes = converter.ConvertHtml(html_content, \"\")\n",
        "\n",
        "# output_file_path = f\"{SHORTCUT}/{EDA_REPORT_FILENAME}.pdf\"\n",
        "# System.IO.File.WriteAllBytes(output_file_path, pdf_bytes)\n"
      ],
      "metadata": {
        "id": "7PWmQKa3nUcR"
      },
      "id": "7PWmQKa3nUcR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report"
      ],
      "metadata": {
        "id": "nNsiRFxdmxUr"
      },
      "id": "nNsiRFxdmxUr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_missing(df)"
      ],
      "metadata": {
        "id": "n1ipuIl9M1U0"
      },
      "id": "n1ipuIl9M1U0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_correlation(df)"
      ],
      "metadata": {
        "id": "_nPqiCeCNHUo"
      },
      "id": "_nPqiCeCNHUo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8f64a90",
      "metadata": {
        "id": "f8f64a90"
      },
      "outputs": [],
      "source": [
        "# Datetime\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "df.sort_values('timestamp', inplace=True)\n",
        "\n",
        "\n",
        "# Calculate day of the week\n",
        "df['day_of_week'] = df['timestamp'].dt.day_name()\n",
        "\n",
        "# Labelencode day_of_week\n",
        "label_encoder = LabelEncoder()\n",
        "df['day_of_week_encoded'] = label_encoder.fit_transform(df['day_of_week'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76b0b810",
      "metadata": {
        "id": "76b0b810"
      },
      "outputs": [],
      "source": [
        "# Missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values:\\n\", missing_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b128814",
      "metadata": {
        "id": "3b128814"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60452d02",
      "metadata": {
        "id": "60452d02"
      },
      "outputs": [],
      "source": [
        "# Distribution of Overall Sleep Score\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['overall_score'], kde=True, bins=20)\n",
        "plt.title('Overall Sleep Score Distribution')\n",
        "plt.xlabel('Overall Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aead249f",
      "metadata": {
        "id": "aead249f"
      },
      "outputs": [],
      "source": [
        "## Correlation Heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b45bfc0",
      "metadata": {
        "id": "9b45bfc0"
      },
      "source": [
        "Notes\n",
        "\n",
        "- Target variable: 'overall_score'\n",
        "- 'deep_sleep_in_minutes' has a high correlation of 0.70\n",
        "- 'restlessness' has a notable negative correlation of -0.40"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Deep Sleep\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['deep_sleep_in_minutes'], kde=True, bins=20)\n",
        "plt.title('Deep Sleep (in minutes) Distribution')\n",
        "plt.xlabel('Minutes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yooiN_4fDdit"
      },
      "id": "yooiN_4fDdit",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Restlessness\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['restlessness'], kde=True, bins=20)\n",
        "plt.title('Restlessness Distribution')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "31s6PF-2D0fT"
      },
      "id": "31s6PF-2D0fT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering\n",
        "\n",
        "- Add lagged features\n",
        "- Add noise"
      ],
      "metadata": {
        "id": "dUbP3BdkExwf"
      },
      "id": "dUbP3BdkExwf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Lag features for 'deep_sleep_in_minutes', 'overall_score', and 'restlessness' (target + 2 important features)\n",
        "for lag in range(1, 8):\n",
        "    df[f'deep_sleep_in_minutes_lag{lag}'] = df['deep_sleep_in_minutes'].shift(lag)\n",
        "    df[f'overall_score_lag{lag}'] = df['overall_score'].shift(lag)\n",
        "    df[f'restlessness_lag{lag}'] = df['restlessness'].shift(lag)\n",
        "\n",
        "# Drop rows with NaN values created by lagging\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "ZoEMS_pb3oDZ"
      },
      "id": "ZoEMS_pb3oDZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise_strength = 0.02\n",
        "features_to_noise = ['deep_sleep_in_minutes', 'resting_heart_rate', 'restlessness'] + \\\n",
        "                    [f'deep_sleep_in_minutes_lag{lag}' for lag in range(1, 8)] + \\\n",
        "                    [f'restlessness_lag{lag}' for lag in range(1, 8)]\n",
        "for feature in features_to_noise:\n",
        "    df[feature] += np.random.normal(0, noise_strength, df.shape[0])"
      ],
      "metadata": {
        "id": "bKl-yu9w3oFr"
      },
      "id": "bKl-yu9w3oFr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM - Deep Learning Neural Network"
      ],
      "metadata": {
        "id": "eWxzem6HbihO"
      },
      "id": "eWxzem6HbihO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sequence length and predictive horizon\n",
        "sequence_length = 7\n",
        "predictive_horizon = 1\n",
        "\n",
        "# Set target, features, and exclude 'revitalization_score' due to 1.0 correlation with target\n",
        "features_columns = ['deep_sleep_in_minutes', 'resting_heart_rate', 'restlessness'] + \\\n",
        "                   [f'deep_sleep_in_minutes_lag{lag}' for lag in range(1, 8)] + \\\n",
        "                   [f'overall_score_lag{lag}' for lag in range(1, 8)] + \\\n",
        "                   [f'restlessness_lag{lag}' for lag in range(1, 8)]\n",
        "\n",
        "features = df[features_columns]\n",
        "target = df['overall_score']\n",
        "\n",
        "# Normalize the features\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Generate sequences for feedforward neural network\n",
        "for i in range(len(scaled_features) - sequence_length - predictive_horizon + 1):\n",
        "    X.append(scaled_features[i:i + sequence_length].flatten())\n",
        "    y.append(target.iloc[i + sequence_length + predictive_horizon - 1])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Define the feedforward neural network model\n",
        "model = Sequential()\n",
        "\n",
        "# Dense layers\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse', 'mae'])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=150, batch_size=32, validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_mse, val_mae = model.evaluate(X_val, y_val)\n",
        "\n",
        "print(f\"Validation Loss: {val_loss}\")\n",
        "print(f\"Validation Mean Squared Error: {val_mse}\")\n",
        "print(f\"Validation Mean Absolute Error: {val_mae}\")"
      ],
      "metadata": {
        "id": "ohL_5y9dbfWb"
      },
      "id": "ohL_5y9dbfWb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing Validation and MAE"
      ],
      "metadata": {
        "id": "Xf5-_UWfbuVN"
      },
      "id": "Xf5-_UWfbuVN"
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training & validation mean absolute error values\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(history.history['mae'], label='Training MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Mean Absolute Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gh_UgKlsbrzQ"
      },
      "id": "gh_UgKlsbrzQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "oMybpoW_cGU0"
      },
      "id": "oMybpoW_cGU0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to create the model\n",
        "def create_model(units_layer1=64, dropout_rate=0.5, units_layer2=32, optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units_layer1, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(units_layer2, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse', 'mae'])\n",
        "    return model\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'units_layer1': [32, 64, 128],\n",
        "    'dropout_rate': [0.3, 0.5, 0.7],\n",
        "    'units_layer2': [16, 32, 64],\n",
        "    'optimizer': ['adam', 'rmsprop']\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "best_model = None\n",
        "best_val_mse = float('inf')\n",
        "\n",
        "for units_layer1 in param_grid['units_layer1']:\n",
        "    for dropout_rate in param_grid['dropout_rate']:\n",
        "        for units_layer2 in param_grid['units_layer2']:\n",
        "            for optimizer in param_grid['optimizer']:\n",
        "                # Create the model\n",
        "                model = create_model(units_layer1, dropout_rate, units_layer2, optimizer)\n",
        "\n",
        "                # Train the model\n",
        "                model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "\n",
        "                # Evaluate on validation set\n",
        "                y_pred = model.predict(X_val)\n",
        "                val_mse = mean_squared_error(y_val, y_pred)\n",
        "\n",
        "                # Check if it's the best model\n",
        "                if val_mse < best_val_mse:\n",
        "                    best_val_mse = val_mse\n",
        "                    best_model = model\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters: \", best_model.get_config())\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "y_pred = best_model.predict(X_val)\n",
        "val_mse = mean_squared_error(y_val, y_pred)\n",
        "val_mae = mean_absolute_error(y_val, y_pred)\n",
        "\n",
        "print(f\"Validation Mean Squared Error: {val_mse}\")\n",
        "print(f\"Validation Mean Absolute Error: {val_mae}\")"
      ],
      "metadata": {
        "id": "tmKPzJH6cGe0"
      },
      "id": "tmKPzJH6cGe0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Importantance"
      ],
      "metadata": {
        "id": "9a3MpY5WFEz8"
      },
      "id": "9a3MpY5WFEz8"
    },
    {
      "cell_type": "code",
      "source": [
        "target = df['overall_score']\n",
        "features_columns = ['deep_sleep_in_minutes', 'resting_heart_rate', 'restlessness'] + \\\n",
        "                   [f'deep_sleep_in_minutes_lag{lag}' for lag in range(1, 8)] + \\\n",
        "                   [f'overall_score_lag{lag}' for lag in range(1, 8)]\n",
        "\n",
        "features = df[features_columns]\n",
        "\n",
        "# Feature importances\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(features, target)\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "importances_series = pd.Series(importances, index=features_columns)\n",
        "sorted_importances = importances_series.sort_values(ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=sorted_importances, y=sorted_importances.index)\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Features')\n",
        "plt.title('Random Forest - Feature Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cYjw8XsTCj8j"
      },
      "id": "cYjw8XsTCj8j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bin deep sleep based on z-scores\n",
        "mean = df['deep_sleep_in_minutes'].mean()\n",
        "std_dev = df['deep_sleep_in_minutes'].std()\n",
        "df['z_score'] = (df['deep_sleep_in_minutes'] - mean) / std_dev\n",
        "df['deep_sleep_category'] = df['z_score'].apply(lambda z: 'Poor' if z < -1 else ('Average' if z < 1 else 'Good'))\n",
        "\n",
        "# Bin restlessness based on z-scores\n",
        "mean_restlessness = df['restlessness'].mean()\n",
        "std_dev_restlessness = df['restlessness'].std()\n",
        "df['z_score_restlessness'] = (df['restlessness'] - mean_restlessness) / std_dev_restlessness\n",
        "df['restlessness_category'] = df['z_score_restlessness'].apply(lambda z: 'Low' if z < -1 else ('Average' if z < 1 else 'High'))"
      ],
      "metadata": {
        "id": "fdniSApCHeMZ"
      },
      "id": "fdniSApCHeMZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Deep Sleep - Categorical Variable\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.histplot(df['deep_sleep_category'], kde=False, bins=20)\n",
        "plt.title('Deep Sleep - Categorical Distribution')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jc1aIUiCHhpn"
      },
      "id": "Jc1aIUiCHhpn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of the restlessness categories\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.histplot(df['restlessness_category'], kde=False, bins=20)\n",
        "plt.title('Restlessness - Categorical Distribution')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pJ9JNhbxWD8S"
      },
      "id": "pJ9JNhbxWD8S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traditional ML Classifier"
      ],
      "metadata": {
        "id": "GIXa1DYyb9a8"
      },
      "id": "GIXa1DYyb9a8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the new categorical variables\n",
        "label_encoder_deep_sleep = LabelEncoder()\n",
        "df['deep_sleep_category_encoded'] = label_encoder_deep_sleep.fit_transform(df['deep_sleep_category'])\n",
        "\n",
        "label_encoder_restlessness = LabelEncoder()\n",
        "df['restlessness_category_encoded'] = label_encoder_restlessness.fit_transform(df['restlessness_category'])"
      ],
      "metadata": {
        "id": "GBYeXfVrcOON"
      },
      "id": "GBYeXfVrcOON",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep_sleep_category classification\n",
        "deep_sleep_features_columns = ['resting_heart_rate', 'restlessness'] + \\\n",
        "                              [f'overall_score_lag{lag}' for lag in range(1, 8)]\n",
        "\n",
        "deep_sleep_features = df[deep_sleep_features_columns]\n",
        "deep_sleep_target = df['deep_sleep_category_encoded']\n",
        "\n",
        "# Split\n",
        "X_train_deep_sleep, X_val_deep_sleep, y_train_deep_sleep, y_val_deep_sleep = train_test_split(\n",
        "    deep_sleep_features,\n",
        "    deep_sleep_target,\n",
        "    test_size=0.2,\n",
        "    shuffle=False,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "deep_sleep_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "deep_sleep_classifier.fit(X_train_deep_sleep, y_train_deep_sleep)\n",
        "\n",
        "y_val_pred_deep_sleep = deep_sleep_classifier.predict(X_val_deep_sleep)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(\"Classification report for deep_sleep_category:\")\n",
        "print(classification_report(y_val_deep_sleep, y_val_pred_deep_sleep))\n",
        "print(\"Confusion matrix for deep_sleep_category:\")\n",
        "print(confusion_matrix(y_val_deep_sleep, y_val_pred_deep_sleep))"
      ],
      "metadata": {
        "id": "C8YIGy1zcOQb"
      },
      "id": "C8YIGy1zcOQb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restlessness_category classification\n",
        "restlessness_features_columns = ['resting_heart_rate', 'deep_sleep_in_minutes'] + \\\n",
        "                                [f'overall_score_lag{lag}' for lag in range(1, 8)]\n",
        "\n",
        "restlessness_features = df[restlessness_features_columns]\n",
        "restlessness_target = df['restlessness_category_encoded']\n",
        "\n",
        "# Split\n",
        "X_train_restlessness, X_val_restlessness, y_train_restlessness, y_val_restlessness = train_test_split(\n",
        "    restlessness_features,\n",
        "    restlessness_target,\n",
        "    test_size=0.2,\n",
        "    shuffle=False,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "restlessness_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "restlessness_classifier.fit(X_train_restlessness, y_train_restlessness)\n",
        "\n",
        "y_val_pred_restlessness = restlessness_classifier.predict(X_val_restlessness)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(\"Classification report for restlessness_category:\")\n",
        "print(classification_report(y_val_restlessness, y_val_pred_restlessness))\n",
        "print(\"Confusion matrix for restlessness_category:\")\n",
        "print(confusion_matrix(y_val_restlessness, y_val_pred_restlessness))"
      ],
      "metadata": {
        "id": "HYq3JLYncOSX"
      },
      "id": "HYq3JLYncOSX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO:\n",
        "\n",
        "Idea: try to predict how you're going to sleep the next day based on prior data.\n",
        "\n",
        "Smart Watch Sleep Tracker - you're predicted to sleep poorly.\n",
        "Time Series Predictor\n",
        "\n",
        "Level of Deep Sleep was low (last night)\n",
        "Be on the look outout\n",
        "\n",
        "## BRANDON\n",
        "\n",
        "* [ ] Refining the model and the CV.\n",
        "\n",
        "## ALEC\n",
        "\n",
        "* [ ] Do RF technique\n",
        "* [ ] First part of documentation\n",
        "* [ ] Classification work\n",
        "\n",
        "## DIAGRAMS\n",
        "\n",
        "1. [ ] DIAGRAM 1: ML1. LOSS GRAPHS\n",
        "2. [ ] DIAGRAM 2: ML2. MAE\n",
        "3. [ ] DIAGRAM 3: SUMMARY GRAPH - whatever major feature\n",
        "4. [ ] DIAGRAM 4: STATUS GRAPH - current status of IOT device (perhaps next day)\n",
        "\n",
        "\n",
        "[ ] Create system diagram\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Similar to Assignment 5, your submitted Tableau Public Dashboard must include:\n",
        "- At least one status visualization - This visualization should tell the dashboard user something about the \"current\" status of their IoT device (e.g. number of devices online, current glucose level, etc.)\n",
        "\n",
        "- At least one summary visualization - This visualization should tell the dashboard user\n",
        "something about the historical data from their IoT device (e.g. average device downtime\n",
        "over the last week, number of hypo/hyperglycemic readings over the last week)\n",
        "- At least one visualization for each of your two machine learning insights -\n",
        "This visualization should communicate the insights created by your machine\n",
        "learning methods.\n"
      ],
      "metadata": {
        "id": "cb6H_xWm8osr"
      },
      "id": "cb6H_xWm8osr"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}